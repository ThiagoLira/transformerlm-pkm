{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "#parser.add_argument('--nhid', type=int, default=200,\n",
    "#                    help='number of hidden units per layer')\n",
    "\n",
    "#parser.add_argument('--ninp', type=int, default=200,\n",
    "#                    help='size of word embeddings')#\n",
    "\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0.0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "    \n",
    "class TransformerCell(nn.Module):\n",
    "        \"\"\"Transformer Cell with Attention and PositionwiseFeedForward\"\"\"\n",
    "        def __init__(self, ntoken, d_model, nff, nheads=8,dropout=0.2, tie_weights=False):\n",
    "            super(TransformerCell, self).__init__()\n",
    "            \n",
    "            self.att = nn.MultiheadAttention(d_model,num_heads=nheads,dropout=dropout)\n",
    "            self.ff = PositionwiseFeedForward(d_model,nff)\n",
    "        \n",
    "            self.norm = LayerNorm(d_model)\n",
    "            self.dropnorm1 = nn.Dropout(dropout)\n",
    "            \n",
    "            self.norm2 = LayerNorm(d_model)\n",
    "            self.dropnorm2 = nn.Dropout(dropout)\n",
    "            \n",
    "        def forward(self,x):\n",
    "            \n",
    "            x,_= self.att(x,x,x)\n",
    "            \n",
    "            x = x + self.dropnorm1(self.norm(x))\n",
    "            \n",
    "            x = self.ff(x)\n",
    "            x = x + self.dropnorm2(self.norm2(x))\n",
    "            \n",
    "            return x\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, ntoken, d_model,nheads=8,nff=512, nlayers=1, dropout=0.0, tie_weights=False):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.emb = Embeddings(d_model,ntoken)\n",
    "        self.pos = PositionalEncoding(d_model,dropout)\n",
    "        \n",
    "        assert(d_model%nheads==0)\n",
    "        \n",
    "        self.trans = TransformerCell(ntoken, d_model, nff=nff, nheads=nheads,dropout=0.2, tie_weights=True)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.Linear(d_model, ntoken),nn.LogSoftmax())\n",
    "        \n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            self.decoder.weight = self.emb.lut.weight\n",
    "\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        emb = self.pos(self.emb(input))\n",
    "        \n",
    "        output =  self.trans(emb)\n",
    "        \n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        \n",
    "        #return decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "\n",
    "# get_batch subdivides the source data into chunks of length args.bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len2 = min(seq_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len2]\n",
    "    target = source[i+1:i+1+seq_len2].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, seq_len):\n",
    "            input_data, targets = get_batch(data_source, i)\n",
    "            output = model(input_data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(input_data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    \n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
    "        input_data, targets = get_batch(train_data, i)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        #print(output.view(-1, ntokens).shape,targets.shape)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % 200 == 0 and batch > 0:\n",
    "            cur_loss = total_loss / 200\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches |  ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // seq_len,\n",
    "                elapsed * 1000 / 200, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "import data\n",
    "import model\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "eval_batch_size = 40\n",
    "\n",
    "batch_size = 100\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/pokeCorpusBulba')\n",
    "\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "\n",
    "seq_len = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overfit one batch\n",
    "\n",
    "\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "\n",
    "model = TransformerModel(ntokens, 1024).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "input_data, targets = get_batch(train_data, 1)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "output = None\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(input_data)\n",
    "\n",
    "    loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i%10 == 0):\n",
    "        print(loss.detach())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiago/gp/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1064 batches |  ms/batch 109.58 | loss  5.79 | ppl   326.92\n",
      "| epoch   1 |   400/ 1064 batches |  ms/batch 106.37 | loss  4.44 | ppl    84.89\n",
      "| epoch   1 |   600/ 1064 batches |  ms/batch 104.61 | loss  3.89 | ppl    48.69\n",
      "| epoch   1 |   800/ 1064 batches |  ms/batch 104.66 | loss  3.57 | ppl    35.36\n",
      "| epoch   1 |  1000/ 1064 batches |  ms/batch 104.70 | loss  3.35 | ppl    28.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 112.67s | valid loss  2.85 | valid ppl    17.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1064 batches |  ms/batch 105.20 | loss  3.12 | ppl    22.61\n",
      "| epoch   2 |   400/ 1064 batches |  ms/batch 104.74 | loss  2.97 | ppl    19.41\n",
      "| epoch   2 |   600/ 1064 batches |  ms/batch 104.81 | loss  2.85 | ppl    17.29\n",
      "| epoch   2 |   800/ 1064 batches |  ms/batch 104.77 | loss  2.76 | ppl    15.73\n",
      "| epoch   2 |  1000/ 1064 batches |  ms/batch 104.87 | loss  2.68 | ppl    14.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 111.57s | valid loss  2.50 | valid ppl    12.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1064 batches |  ms/batch 105.26 | loss  2.61 | ppl    13.53\n",
      "| epoch   3 |   400/ 1064 batches |  ms/batch 104.77 | loss  2.53 | ppl    12.61\n",
      "| epoch   3 |   600/ 1064 batches |  ms/batch 104.77 | loss  2.48 | ppl    11.92\n",
      "| epoch   3 |   800/ 1064 batches |  ms/batch 104.74 | loss  2.43 | ppl    11.35\n",
      "| epoch   3 |  1000/ 1064 batches |  ms/batch 104.86 | loss  2.39 | ppl    10.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 111.65s | valid loss  2.29 | valid ppl     9.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 1064 batches |  ms/batch 105.93 | loss  2.36 | ppl    10.56\n",
      "| epoch   4 |   400/ 1064 batches |  ms/batch 104.98 | loss  2.31 | ppl    10.08\n",
      "| epoch   4 |   600/ 1064 batches |  ms/batch 104.76 | loss  2.27 | ppl     9.67\n",
      "| epoch   4 |   800/ 1064 batches |  ms/batch 104.81 | loss  2.24 | ppl     9.39\n",
      "| epoch   4 |  1000/ 1064 batches |  ms/batch 104.77 | loss  2.21 | ppl     9.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 111.74s | valid loss  2.18 | valid ppl     8.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 1064 batches |  ms/batch 105.26 | loss  2.20 | ppl     8.99\n",
      "| epoch   5 |   400/ 1064 batches |  ms/batch 104.78 | loss  2.16 | ppl     8.69\n",
      "| epoch   5 |   600/ 1064 batches |  ms/batch 104.86 | loss  2.13 | ppl     8.42\n",
      "| epoch   5 |   800/ 1064 batches |  ms/batch 104.82 | loss  2.11 | ppl     8.23\n",
      "| epoch   5 |  1000/ 1064 batches |  ms/batch 104.78 | loss  2.09 | ppl     8.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 111.59s | valid loss  2.15 | valid ppl     8.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 1064 batches |  ms/batch 105.27 | loss  2.08 | ppl     8.01\n",
      "| epoch   6 |   400/ 1064 batches |  ms/batch 104.76 | loss  2.05 | ppl     7.80\n",
      "| epoch   6 |   600/ 1064 batches |  ms/batch 104.84 | loss  2.02 | ppl     7.57\n",
      "| epoch   6 |   800/ 1064 batches |  ms/batch 104.80 | loss  2.01 | ppl     7.45\n",
      "| epoch   6 |  1000/ 1064 batches |  ms/batch 104.82 | loss  1.99 | ppl     7.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 111.59s | valid loss  2.09 | valid ppl     8.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 1064 batches |  ms/batch 105.31 | loss  1.99 | ppl     7.34\n",
      "| epoch   7 |   400/ 1064 batches |  ms/batch 104.81 | loss  1.97 | ppl     7.15\n",
      "| epoch   7 |   600/ 1064 batches |  ms/batch 104.76 | loss  1.95 | ppl     7.01\n",
      "| epoch   7 |   800/ 1064 batches |  ms/batch 104.73 | loss  1.93 | ppl     6.91\n",
      "| epoch   7 |  1000/ 1064 batches |  ms/batch 104.77 | loss  1.92 | ppl     6.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 111.57s | valid loss  2.02 | valid ppl     7.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 1064 batches |  ms/batch 105.29 | loss  1.92 | ppl     6.85\n",
      "| epoch   8 |   400/ 1064 batches |  ms/batch 104.81 | loss  1.90 | ppl     6.72\n",
      "| epoch   8 |   600/ 1064 batches |  ms/batch 106.61 | loss  1.88 | ppl     6.58\n",
      "| epoch   8 |   800/ 1064 batches |  ms/batch 109.79 | loss  1.87 | ppl     6.50\n",
      "| epoch   8 |  1000/ 1064 batches |  ms/batch 106.44 | loss  1.86 | ppl     6.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 113.48s | valid loss  2.03 | valid ppl     7.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 1064 batches |  ms/batch 106.82 | loss  1.87 | ppl     6.48\n",
      "| epoch   9 |   400/ 1064 batches |  ms/batch 107.18 | loss  1.85 | ppl     6.34\n",
      "| epoch   9 |   600/ 1064 batches |  ms/batch 109.20 | loss  1.83 | ppl     6.24\n",
      "| epoch   9 |   800/ 1064 batches |  ms/batch 106.32 | loss  1.82 | ppl     6.19\n",
      "| epoch   9 |  1000/ 1064 batches |  ms/batch 109.93 | loss  1.81 | ppl     6.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 115.08s | valid loss  1.95 | valid ppl     7.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 1064 batches |  ms/batch 113.09 | loss  1.82 | ppl     6.20\n",
      "| epoch  10 |   400/ 1064 batches |  ms/batch 108.96 | loss  1.81 | ppl     6.09\n",
      "| epoch  10 |   600/ 1064 batches |  ms/batch 110.77 | loss  1.79 | ppl     5.97\n",
      "| epoch  10 |   800/ 1064 batches |  ms/batch 113.19 | loss  1.78 | ppl     5.94\n",
      "| epoch  10 |  1000/ 1064 batches |  ms/batch 110.25 | loss  1.77 | ppl     5.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 118.54s | valid loss  1.91 | valid ppl     6.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 1064 batches |  ms/batch 108.57 | loss  1.78 | ppl     5.95\n",
      "| epoch  11 |   400/ 1064 batches |  ms/batch 107.79 | loss  1.77 | ppl     5.87\n",
      "| epoch  11 |   600/ 1064 batches |  ms/batch 110.24 | loss  1.75 | ppl     5.78\n",
      "| epoch  11 |   800/ 1064 batches |  ms/batch 108.92 | loss  1.75 | ppl     5.73\n",
      "| epoch  11 |  1000/ 1064 batches |  ms/batch 107.56 | loss  1.74 | ppl     5.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 115.45s | valid loss  1.90 | valid ppl     6.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/ 1064 batches |  ms/batch 109.27 | loss  1.75 | ppl     5.77\n",
      "| epoch  12 |   400/ 1064 batches |  ms/batch 108.42 | loss  1.73 | ppl     5.66\n",
      "| epoch  12 |   600/ 1064 batches |  ms/batch 112.24 | loss  1.72 | ppl     5.58\n",
      "| epoch  12 |   800/ 1064 batches |  ms/batch 113.52 | loss  1.72 | ppl     5.56\n",
      "| epoch  12 |  1000/ 1064 batches |  ms/batch 113.65 | loss  1.71 | ppl     5.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 118.55s | valid loss  1.90 | valid ppl     6.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/ 1064 batches |  ms/batch 112.81 | loss  1.72 | ppl     5.59\n",
      "| epoch  13 |   400/ 1064 batches |  ms/batch 113.96 | loss  1.71 | ppl     5.52\n",
      "| epoch  13 |   600/ 1064 batches |  ms/batch 113.91 | loss  1.69 | ppl     5.44\n",
      "| epoch  13 |   800/ 1064 batches |  ms/batch 110.66 | loss  1.69 | ppl     5.42\n",
      "| epoch  13 |  1000/ 1064 batches |  ms/batch 110.15 | loss  1.68 | ppl     5.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 119.27s | valid loss  1.91 | valid ppl     6.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 1064 batches |  ms/batch 108.19 | loss  1.70 | ppl     5.46\n",
      "| epoch  14 |   400/ 1064 batches |  ms/batch 108.66 | loss  1.68 | ppl     5.38\n",
      "| epoch  14 |   600/ 1064 batches |  ms/batch 108.31 | loss  1.67 | ppl     5.32\n",
      "| epoch  14 |   800/ 1064 batches |  ms/batch 108.69 | loss  1.67 | ppl     5.29\n",
      "| epoch  14 |  1000/ 1064 batches |  ms/batch 107.36 | loss  1.66 | ppl     5.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 114.96s | valid loss  1.85 | valid ppl     6.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 1064 batches |  ms/batch 108.31 | loss  1.68 | ppl     5.34\n",
      "| epoch  15 |   400/ 1064 batches |  ms/batch 107.05 | loss  1.67 | ppl     5.29\n",
      "| epoch  15 |   600/ 1064 batches |  ms/batch 108.35 | loss  1.65 | ppl     5.19\n",
      "| epoch  15 |   800/ 1064 batches |  ms/batch 106.37 | loss  1.64 | ppl     5.18\n",
      "| epoch  15 |  1000/ 1064 batches |  ms/batch 109.23 | loss  1.64 | ppl     5.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 114.73s | valid loss  1.86 | valid ppl     6.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/ 1064 batches |  ms/batch 107.86 | loss  1.66 | ppl     5.24\n",
      "| epoch  16 |   400/ 1064 batches |  ms/batch 110.02 | loss  1.64 | ppl     5.18\n",
      "| epoch  16 |   600/ 1064 batches |  ms/batch 109.62 | loss  1.63 | ppl     5.11\n",
      "| epoch  16 |   800/ 1064 batches |  ms/batch 108.68 | loss  1.63 | ppl     5.09\n",
      "| epoch  16 |  1000/ 1064 batches |  ms/batch 107.75 | loss  1.63 | ppl     5.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 115.62s | valid loss  1.85 | valid ppl     6.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/ 1064 batches |  ms/batch 107.30 | loss  1.64 | ppl     5.15\n",
      "| epoch  17 |   400/ 1064 batches |  ms/batch 105.75 | loss  1.63 | ppl     5.08\n",
      "| epoch  17 |   600/ 1064 batches |  ms/batch 107.63 | loss  1.61 | ppl     5.02\n",
      "| epoch  17 |   800/ 1064 batches |  ms/batch 108.46 | loss  1.61 | ppl     5.01\n",
      "| epoch  17 |  1000/ 1064 batches |  ms/batch 108.00 | loss  1.61 | ppl     5.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 114.64s | valid loss  1.80 | valid ppl     6.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/ 1064 batches |  ms/batch 106.93 | loss  1.62 | ppl     5.08\n",
      "| epoch  18 |   400/ 1064 batches |  ms/batch 107.15 | loss  1.61 | ppl     5.01\n",
      "| epoch  18 |   600/ 1064 batches |  ms/batch 109.42 | loss  1.60 | ppl     4.94\n",
      "| epoch  18 |   800/ 1064 batches |  ms/batch 109.27 | loss  1.60 | ppl     4.95\n",
      "| epoch  18 |  1000/ 1064 batches |  ms/batch 107.55 | loss  1.59 | ppl     4.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 115.22s | valid loss  1.80 | valid ppl     6.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/ 1064 batches |  ms/batch 110.46 | loss  1.61 | ppl     5.00\n",
      "| epoch  19 |   400/ 1064 batches |  ms/batch 109.08 | loss  1.60 | ppl     4.94\n",
      "| epoch  19 |   600/ 1064 batches |  ms/batch 107.72 | loss  1.59 | ppl     4.88\n",
      "| epoch  19 |   800/ 1064 batches |  ms/batch 106.72 | loss  1.59 | ppl     4.88\n",
      "| epoch  19 |  1000/ 1064 batches |  ms/batch 108.11 | loss  1.58 | ppl     4.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 115.30s | valid loss  1.78 | valid ppl     5.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 1064 batches |  ms/batch 112.72 | loss  1.60 | ppl     4.94\n",
      "| epoch  20 |   400/ 1064 batches |  ms/batch 111.52 | loss  1.59 | ppl     4.89\n",
      "| epoch  20 |   600/ 1064 batches |  ms/batch 109.27 | loss  1.57 | ppl     4.82\n",
      "| epoch  20 |   800/ 1064 batches |  ms/batch 111.32 | loss  1.57 | ppl     4.82\n",
      "| epoch  20 |  1000/ 1064 batches |  ms/batch 110.29 | loss  1.57 | ppl     4.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 118.05s | valid loss  1.78 | valid ppl     5.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/ 1064 batches |  ms/batch 112.12 | loss  1.59 | ppl     4.89\n",
      "| epoch  21 |   400/ 1064 batches |  ms/batch 109.44 | loss  1.58 | ppl     4.83\n",
      "| epoch  21 |   600/ 1064 batches |  ms/batch 112.54 | loss  1.56 | ppl     4.78\n",
      "| epoch  21 |   800/ 1064 batches |  ms/batch 113.61 | loss  1.56 | ppl     4.75\n",
      "| epoch  21 |  1000/ 1064 batches |  ms/batch 108.63 | loss  1.56 | ppl     4.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 118.38s | valid loss  1.73 | valid ppl     5.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/ 1064 batches |  ms/batch 113.55 | loss  1.57 | ppl     4.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "\n",
    "model = TransformerModel(ntokens, 512).to(device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "        \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, 100):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "n_words = 100\n",
    "corpus = data.Corpus('./data/pokeCorpusBulba')\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiago/gp/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"ash and his friends\"\n",
    "\n",
    "indices = [corpus.dictionary.word2idx[w] for w in input_sentence.split(' ')]\n",
    "\n",
    "input = torch.LongTensor(indices).to(device).view(1,-1)\n",
    "output= model(input)\n",
    "#torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiago/gp/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon pokémon "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  # no tracking history\n",
    "        for i in range(n_words+50):\n",
    "            \n",
    "            output= model(input)\n",
    "            \n",
    "            word_weights = output.squeeze().exp().cpu()\n",
    "            \n",
    "            word_idx = torch.multinomial(word_weights, 1)[-1].to(device)\n",
    "            \n",
    "            input = torch.cat((input.squeeze(),word_idx)).view(1,-1)\n",
    "            \n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "            print(word,end =\" \") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5,   6,   7,   8, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153,\n",
       "         153, 153]], device='cuda:0')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".gp",
   "language": "python",
   "name": ".gp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
